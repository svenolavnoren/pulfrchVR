Mål med projektet

bakgrund

när jag köpte min insta 360 fanns den här idén att göra egna 3d360-bilder redan undermedvetet. jag vet inte när idén föddes. som barn fick jag en leksak som hette pop-hop och världarna där fa1cinerade mig och det tog ett år tills jag en dag runt 1971 kom på mig med att "rev-enga" principen mentalt. Där var ju tydligen två bilder, en för varje öga, ahhh...

2017 (ungerfär) kom och 3d-glasögon var "årets julklapp". Jag lekte redan då lite på egen hand med google cardboard och min gamla S4 mini. Innan dess hade retat mig lite över hur onödigt det är med FHD och ännu högre på den tidens "statusmobiler. Men då insåg jag att en sån där dyr mobil med massa pixlar och mjuka gyron skulle sitta fint. Jag fick det bekräftat ganska snart då jag blev lycklig ägare av en modernare mobil med starkare arm-dator, FHD och mjuka gyron. Det här var kanske 1019 och Google cardboard började bli kul på riktigt, men omvärlden tycktes då redan tappat lusten för det här med vardags-VR. Mitt intresse fanns kvar fast bara latent. Sporadiskt skapade jag 360-foton med mobilen och en stiching-app. Kul men inget man orkar hålla på med och dessutom blev det sällan riktigt bra med stitchandet.

En vacker dag 2024 kom suget tillbaka, jag googlade på 360-kamereor. Kort därefter ägde jag en insta360x4 som visserligen kostade ett antal tusen men jag tyckte det var fantastiskt billigt. Fotade, filmade och hade kul med det. Kom så småningom tillbaka till grundiden från "pop-hop". Tog två bilder, tog fram microsoft paint, svor i en timme och det kändes som jag hade tummen mitt i handen. Till sist hade jag i alla fall lyckats editera fram en 3d360 top-bottom bild. Stoppade in denna i mobilen (nu en modern mobil med fhd och mjuka gyron) och tittae i cardboard. Wow! Helt häpen. Min low tech-lösning är ju verkligen något. Fast fortfarande inte så bra. men nu finns ju meta quest 3... ska jag "slänga ut" 7000 kronor? Det var en chansning. Hämtade ut denna, in med 360-videor, och 360-bilder. Gäss! Här fortsätter vi. Det här är riktigt bra.

Vanlig 2d360 lekte jag med och det var kul, men nåt är fel. Det blir vackert, men VR-känslan är inte där, inte så kul som jag hoppades ändå. Så jag tänkte ut hur man kan göra video och förskjuta med tidsfördröjning vilket visade sig vara trivialt med hjälp av ffmpeg. En vacker vårdag 2025 ville jag visa och två vänner kom hem till mig. När jag förberedde bildvisningen slog det mig att vi är nog alla mer eller mindre "allergiska mot skitstereo". Och jag insåg hur bra mina stillbilder kändes, jämfört med videorona. Detta fick jag också bekräftat vid bildvisningen. Så min strategi blev att jobba mot "bra stillbilder" istället för "häftiga" rörliga bilder. Skitstereo måste få bli undantag och det kräver väldigt mycket av både pulfrichfotografen och scenen.

Då föddes home3dphotos.pas, som gjorde det hela "automatiskt". Nåväl, halvautomatiskt. På hösten 2025 hittade jag till chatgpt. Jag hoppades kunna få hjälp med att gå i mål med det här projektet. Vad som saknas är att göra det hela så helautomatiskt så att en lekman ges chans att jobba med bildskapandet utan att behöva fastna i någon av alla fällor på vägen mellan kamera och headset. Jag skulle vilja ha vettigt UI för hela processen. Min tanke till implementation var att använda befintlig home3d_photos.pas och bygga ett UI för det första viktiga bildurvalssteget. Det började bli jobbigt i längden även när jag blev van att dumpa videon till png, leta fram bilderna, för att manuellt plocka upp dessa i home3d_photos. I skrivande stund är det 27.feb 2026 och den version vi har nu gör samma sak, fast på ett lite annorlunda sätt. Hela processen från bildurval till stereobildskapande gör i det som från böjran enbart var tänkt som bildurvalsapp. På plats finns nu förutom bildurvalet även sterobildskapandet. Det som tidigare gjordet i kanske 6 steg har vi lyckats förfina till att det görs i ett enda steg. Vi lyckas nu också med konststycket att gå direkt från videon, gå lossless både inte bara i luminans men också i krominans tack vare RGB24-tricket vi hittade fram till, samt att när vi ändå håller på själva framkallningen inbyggd i form av bat runner. Den tillkom lite av ett misstag, jag sa hur jag tänkte vad jag vill göra i ett längre raljerande, plötlsligt tyckte chatgpt att vi kan ju koda så här och tjoff på plats fanns bat runner. Jag försökte byta namn på den ett antal gånger tills jag fann att jag dess ursprungsnamn sitter som en smäck, tycker jag. Viktig detalj med ui-layouten att den känns lagom nedtonad och fin.

Vi har nosat på sidospår med videor med dubbla kameror och bra stereoljud, insv etc, allt detta är intressant, men hör inte direkt hemma i målet för vår grundapp pulfrichVR. Denna strävar efter att inte skrämma bort vare sig Arne oneshoot Bananberg eller andra bildkonstnärer med hatkärlek till detajer, för att kunna klara sin huvuduppgift att vara så bra som möljigt på sin uppgift 2d360-video -> 3d360-stillbilder.

En detalj jag aldrig nämnde var att jag kostade på mig insta360x5 direkt när den kom och trots en relativt liten förbättring jämfört med x4:an tycker jag den var värd varenda krona. x4:an blev liggande men dammades snart av när jag insåg hur bra gummisnoddsstereo med dubbla kameror faktiskt funkar. Men som sagt video är på kul, fotona är vår huvuduppgift.


Var vi är nu:
Vi har hittat till github. Chatgpt har flera gånger antytt att vi är ett steg från 1.0, medan 1.0 i min värld betyder att den har betatestats ordentligt av andra än mig, och att alla issues identifierats och tagits hänsyn till. Min strategi under "alfa-testningen" har varit att rätta alla fel som dyker upp, och det har funkat fint. Däremot känner jag att koden skulle må bra av antingen en total renskrivning/omskrivning, eller i minsta fall "städning". Målet med det är att slippa alla otäcka datadubleringar som kommit med grundat på det sätt vi byggt på: Jag har specat, chatgpt har kodat enlig det. Vi har sammarbetat jättebra och kan vara stolta över programvarans nuvarande kvalitet ur runtime-synvinkel. Allt "bara funkar" som den känns nu, även om det säkert gömmer sig ett och annat som jag missat vid användning. Men det börjar bli svårt att bygga om/ändra, i och med att vi byggt lite grand "bottom up" utan entydig "top-down"-spec. Vi har top down hyfsat klart för oss nu men det är inte helt lätt att uttrycka i skrift såsom pseudo-kod, efterom det blir olika för mig beroendå på vilket perspektiv jag väljer. En idé om vettigt perspektiv är att speca för exakt insta360x5, insta360 Studio, 8k, "ospecad viewer" men ändå "just-works-kombon" quest3+Skybox VR player som referensmål. Men pulfrichVR.py är tänkt som ett generellt verktyg för 3d360-skapande som ska kunna fungera oavsett kamerea och headset, genom att dess hemmaplan är att hålla sig de tydligt definierade (de-facto) standarderna färdigbalanserad equirect film -> 3d360 stillbild.





